import os
import csv
from collections import Counter
from colorama import init, Fore, Style

init(autoreset=True)

# Basic English stopwords for keyword analysis
STOPWORDS = {
    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',
    'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',
    'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that',
    'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',
    'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',
    'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',
    'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',
    'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',
    'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should',
    'now', 'paper', 'study', 'results', 'research', 'method', 'methods', 'introduction', 'conclusion',
    'abstract', 'figure', 'table', 'data', 'analysis', 'based', 'using', 'also', 'however'
}

def get_unique_filepath(directory, filename):
    """
    Generates a unique file path, adding a numerical suffix if the file already exists.
    Also ensures the target directory exists.
    """
    os.makedirs(directory, exist_ok=True)
    filepath = os.path.join(directory, filename)
    if not os.path.exists(filepath):
        return filepath
    
    # If the file already exists, find a new name
    base, extension = os.path.splitext(filename)
    counter = 1
    while True:
        new_filename = f"{base}-{counter}{extension}"
        new_filepath = os.path.join(directory, new_filename)
        if not os.path.exists(new_filepath):
            return new_filepath
        counter += 1

def display_header(title):
    print(f"\n{Fore.CYAN}{Style.BRIGHT}{'=' * 80}")
    print(f"{Fore.CYAN}{Style.BRIGHT}{title.center(80)}")
    print(f"{Fore.CYAN}{Style.BRIGHT}{'=' * 80}{Style.RESET_ALL}")

def display_top_items(title, counter, top_n=5):
    print(f"\n{Fore.YELLOW}--- {title} (Top {top_n}) ---{Style.RESET_ALL}")
    if not counter:
        print(f"{Fore.WHITE}No data to display.")
        return
    for item, count in counter.most_common(top_n):
        print(f"{Fore.GREEN}{item}: {Style.BRIGHT}{count} mentions")

def save_stats_to_csv(data_list, filename, fieldnames):
    """Saves a list of dictionaries to a CSV file."""
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(data_list)
    print(f"{Fore.GREEN}âœ… Statistics saved to '{filename}'")

# ==============================================================================
# HELPER FUNCTION TO SAVE COUNTER DATA
# ==============================================================================
def save_counter_to_csv(counter_obj, directory, filename, fieldnames):
    """Converts a Counter object to a list of dictionaries and saves it to a CSV file."""
    if not counter_obj:
        return
        
    # Convert Counter to a list of dictionaries
    data_list = [{fieldnames[0]: item, fieldnames[1]: count} for item, count in counter_obj.most_common()]
    
    # Get a unique file path and save the data
    unique_filepath = get_unique_filepath(directory, filename)
    save_stats_to_csv(data_list, unique_filepath, fieldnames)

def analyze_autosearch(folder_path):
    """
    Analyzes the CSV files generated by the search.
    Handles both Semantic Scholar (Title, Year) and ArXiv (title, year) formats.
    Saves results in log/<folder_name>/
    """
    display_header(f"Statistics for '{folder_path}' Folder")
    
    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
    if not csv_files:
        print(f"{Fore.RED}âŒ No CSV files found in '{folder_path}'.")
        return

    year_counts = Counter()
    author_counts = Counter()
    all_papers = []
    processed_titles = set()
    total_articles = 0
    contains_arxiv_data = False

    print(f"Analyzing {len(csv_files)} files...")
    for filename in csv_files:
        filepath = os.path.join(folder_path, filename)
        is_arxiv = filename.startswith("arxiv_")
        
        if is_arxiv:
            contains_arxiv_data = True
            print(f"{Fore.BLUE}â„¹ï¸  Processing ArXiv file: {filename}")
        
        with open(filepath, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # ---------------------------------------------------------
                # NORMALIZE DATA BASED ON SOURCE (ARXIV VS SEMANTIC SCHOLAR)
                # ---------------------------------------------------------
                if is_arxiv:
                    raw_title = row.get("title", "")
                    raw_year = row.get("year", "0")
                    raw_authors = row.get("authors", "")
                    raw_citations = 0 
                    raw_url = row.get("pdf_url", "")
                else:
                    raw_title = row.get("Title", "")
                    raw_year = row.get("Year", "0")
                    raw_authors = row.get("Authors", "")
                    raw_citations = row.get("Citations", 0)
                    raw_url = row.get("URL", "")

                title = raw_title.strip()
                
                # Deduplication logic
                if not title or title.upper() == "N/A":
                    continue
                
                # Create a simple normalized key for deduplication
                title_key = " ".join(title.lower().split())
                if title_key in processed_titles:
                    continue
                
                processed_titles.add(title_key)
                total_articles += 1

                # Update Counters
                if raw_year: 
                    year_counts[raw_year] += 1
                
                if raw_authors: 
                    normalized_authors = raw_authors.replace(";", ",")
                    author_list = [a.strip() for a in normalized_authors.split(',') if a.strip()]
                    author_counts.update(author_list)

                try:
                    all_papers.append({
                        "title": title,
                        "citations": int(raw_citations),
                        "authors": raw_authors,
                        "url": raw_url,
                        "source": "ArXiv" if is_arxiv else "Semantic Scholar"
                    })
                except (ValueError, TypeError):
                    continue

    sorted_papers = sorted(all_papers, key=lambda p: p['citations'], reverse=True)
    
    # -----------------------------------------------------------
    # DEFINE LOG DIRECTORY BASED ON SOURCE FOLDER
    # -----------------------------------------------------------
    folder_name = os.path.basename(os.path.normpath(folder_path))
    log_directory = os.path.join("log", folder_name)
    os.makedirs(log_directory, exist_ok=True)
    
    print(f"\n{Fore.CYAN}ðŸ“‚ Saving statistics to: {Style.BRIGHT}{log_directory}")

    # Save Logs
    if sorted_papers:
        save_stats_to_csv(
            sorted_papers, 
            get_unique_filepath(log_directory, "top_papers.csv"), 
            ["title", "citations", "authors", "url", "source"]
        )
    
    save_counter_to_csv(year_counts, log_directory, "productive_years.csv", ["Year", "Article_Count"])
    save_counter_to_csv(author_counts, log_directory, "prolific_authors.csv", ["Author", "Article_Count"])

    # Output Summary to Console
    print(f"\n{Fore.WHITE}--- Overall Summary ---")
    print(f"{Fore.GREEN}Total Articles Analyzed: {Style.BRIGHT}{total_articles}")
    print(f"{Fore.GREEN}Unique Articles Found: {Style.BRIGHT}{len(processed_titles)}")

    display_top_items("Most Productive Years", year_counts)
    display_top_items("Most Prolific Authors", author_counts)

    print(f"\n{Fore.YELLOW}--- Top {5} Most Cited Papers ---{Style.RESET_ALL}")
    
    if contains_arxiv_data and not any(p['citations'] > 0 for p in sorted_papers):
        print(f"{Fore.WHITE}Note: ArXiv papers do not contain citation data via API.")
        print(f"{Fore.WHITE}Showing most recent papers instead:")
        recent_papers = sorted(all_papers, key=lambda x: x.get('year', 0), reverse=True)[:5]
        for i, paper in enumerate(recent_papers):
             print(f"{Fore.WHITE}{i+1}. {Fore.GREEN}{paper['title'][:67]}... \n   {Style.DIM}{Fore.WHITE}â””â”€ Source: {paper['source']}{Style.RESET_ALL}")
    
    elif not sorted_papers:
        print(f"{Fore.WHITE}No papers found.")
    else:
        for i, paper in enumerate(sorted_papers[:5]):
            title = paper['title']
            if len(title) > 70:
                title = title[:67] + "..."
            
            cit_info = f"Citations: {paper['citations']}" if paper['citations'] > 0 else "Citations: N/A (ArXiv)"
            print(f"{Fore.WHITE}{i+1}. {Fore.GREEN}{title} \n   {Style.DIM}{Fore.WHITE}â””â”€ {cit_info} | Source: {paper['source']}{Style.RESET_ALL}")

def analyze_llama_csv_results(folder_path):
    """
    Analyzes the CSV file generated by the AI filter.
    Saves results in log/<folder_name>/
    """
    display_header("AI Filter Results Statistics")

    csv_file = os.path.join(folder_path, "llama_filtered_articles.csv")
    if not os.path.exists(csv_file):
        print(f"{Fore.RED}âŒ File '{csv_file}' not found. Please run the AI filter first.")
        return

    year_counts = Counter()
    author_counts = Counter()
    all_papers = []
    processed_titles = set()
    total_articles = 0
    
    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            total_articles += 1
            # Check keys
            title = row.get("Title", row.get("title", "")).strip()
            
            if not title or title.upper() == "N/A":
                continue

            title_key = " ".join(title.lower().split())
            if title_key in processed_titles:
                continue

            processed_titles.add(title_key)

            # Handle Year
            year = row.get("Year", row.get("year", ""))
            if year: year_counts[year] += 1
            
            # Handle Authors
            authors = row.get("Authors", row.get("authors", ""))
            if authors: 
                 normalized_authors = authors.replace(";", ",")
                 author_counts.update([a.strip() for a in normalized_authors.split(',') if a.strip()])
            
            try:
                cit_val = row.get("Citations", row.get("citations", 0))
                all_papers.append({
                    "title": title,
                    "citations": int(cit_val),
                    "authors": authors,
                    "url": row.get("URL", row.get("pdf_url", "N/A"))
                })
            except (ValueError, TypeError):
                continue
    
    sorted_papers = sorted(all_papers, key=lambda p: p['citations'], reverse=True)

    # -----------------------------------------------------------
    # DEFINE LOG DIRECTORY BASED ON SOURCE FOLDER
    # -----------------------------------------------------------
    folder_name = os.path.basename(os.path.normpath(folder_path))
    log_directory = os.path.join("log", folder_name)
    os.makedirs(log_directory, exist_ok=True)

    print(f"\n{Fore.CYAN}ðŸ“‚ Saving statistics to: {Style.BRIGHT}{log_directory}")

    # Save Logs
    if sorted_papers:
        save_stats_to_csv(
            sorted_papers, 
            get_unique_filepath(log_directory, "top_papers.csv"), 
            ["title", "citations", "authors", "url"]
        )

    save_counter_to_csv(year_counts, log_directory, "productive_years.csv", ["Year", "Article_Count"])
    save_counter_to_csv(author_counts, log_directory, "prolific_authors.csv", ["Author", "Article_Count"])
    
    print(f"\n{Fore.WHITE}--- Overall Summary ---")
    print(f"{Fore.GREEN}Total Articles Approved by AI: {Style.BRIGHT}{total_articles}")
    print(f"{Fore.GREEN}Unique Articles Found: {Style.BRIGHT}{len(processed_titles)}")

    display_top_items("Most Productive Years", year_counts)
    display_top_items("Most Prolific Authors", author_counts)

    print(f"\n{Fore.YELLOW}--- Top {5} Most Cited Papers ---{Style.RESET_ALL}")
    if not sorted_papers:
        print(f"{Fore.WHITE}No papers with citation data found.")
    else:
        for i, paper in enumerate(sorted_papers[:5]):
            title = paper['title']
            if len(title) > 70:
                title = title[:67] + "..."
            print(f"{Fore.WHITE}{i+1}. {Fore.GREEN}{title} \n   {Style.DIM}{Fore.WHITE}â””â”€ Citations: {Style.BRIGHT}{paper['citations']}{Style.RESET_ALL}")
import os
import csv
from collections import Counter
from colorama import init, Fore, Style

init(autoreset=True)

# Basic English stopwords for keyword analysis
STOPWORDS = {
    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',
    'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',
    'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that',
    'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',
    'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',
    'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',
    'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',
    'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',
    'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should',
    'now', 'paper', 'study', 'results', 'research', 'method', 'methods', 'introduction', 'conclusion',
    'abstract', 'figure', 'table', 'data', 'analysis', 'based', 'using', 'also', 'however'
}

def get_unique_filepath(directory, filename):
    """
    Generates a unique file path, adding a numerical suffix if the file already exists.
    Also ensures the target directory exists.
    """
    os.makedirs(directory, exist_ok=True)
    filepath = os.path.join(directory, filename)
    if not os.path.exists(filepath):
        return filepath
    
    # If the file already exists, find a new name
    base, extension = os.path.splitext(filename)
    counter = 1
    while True:
        new_filename = f"{base}-{counter}{extension}"
        new_filepath = os.path.join(directory, new_filename)
        if not os.path.exists(new_filepath):
            return new_filepath
        counter += 1

def display_header(title):
    print(f"\n{Fore.CYAN}{Style.BRIGHT}{'=' * 80}")
    print(f"{Fore.CYAN}{Style.BRIGHT}{title.center(80)}")
    print(f"{Fore.CYAN}{Style.BRIGHT}{'=' * 80}{Style.RESET_ALL}")

def display_top_items(title, counter, top_n=5):
    print(f"\n{Fore.YELLOW}--- {title} (Top {top_n}) ---{Style.RESET_ALL}")
    if not counter:
        print(f"{Fore.WHITE}No data to display.")
        return
    for item, count in counter.most_common(top_n):
        print(f"{Fore.GREEN}{item}: {Style.BRIGHT}{count} mentions")

def save_stats_to_csv(data_list, filename, fieldnames):
    """Saves a list of dictionaries to a CSV file."""
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(data_list)
    print(f"{Fore.GREEN}✅ Statistics saved to '{filename}'")

# ==============================================================================
# NEW HELPER FUNCTION TO SAVE COUNTER DATA
# ==============================================================================
def save_counter_to_csv(counter_obj, directory, filename, fieldnames):
    """Converts a Counter object to a list of dictionaries and saves it to a CSV file."""
    if not counter_obj:
        return
        
    # Convert Counter to a list of dictionaries
    # E.g., Counter({'2020': 5, '2021': 3}) -> [{'Year': '2020', 'Count': 5}, {'Year': '2021', 'Count': 3}]
    data_list = [{fieldnames[0]: item, fieldnames[1]: count} for item, count in counter_obj.most_common()]
    
    # Get a unique file path and save the data
    unique_filepath = get_unique_filepath(directory, filename)
    save_stats_to_csv(data_list, unique_filepath, fieldnames)

def analyze_autosearch(folder_path):
    """
    Analyzes the CSV files generated by the bibliographic search.
    This function now takes the folder_path as an argument.
    """
    display_header(f"Statistics for '{folder_path}' Folder")
    
    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
    if not csv_files:
        print(f"{Fore.RED}❌ No CSV files found in '{folder_path}'.")
        return

    year_counts = Counter()
    author_counts = Counter()
    all_papers = []
    processed_titles = set()
    total_articles = 0

    print(f"Analyzing {len(csv_files)} files...")
    for filename in csv_files:
        filepath = os.path.join(folder_path, filename)
        with open(filepath, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                total_articles += 1
                title = row.get("Title", "").strip()
                # The logic of `processed_titles` already ensures a unique count per article
                if not title or title.upper() == "N/A" or title in processed_titles:
                    continue

                processed_titles.add(title)
                
                if row.get("Year"): year_counts[row["Year"]] += 1
                if row.get("Authors"): author_counts.update([author.strip() for author in row["Authors"].split(';')])
                
                try:
                    all_papers.append({
                        "title": title,
                        "citations": int(row.get("Citations", 0)),
                        "authors": row.get("Authors", "N/A"),
                        "url": row.get("URL", "N/A")
                    })
                except (ValueError, TypeError):
                    continue

    sorted_papers = sorted(all_papers, key=lambda p: p['citations'], reverse=True)
    
    # ==============================================================================
    # MODIFICATION: Save ALL data for years, authors, and articles
    # ==============================================================================
    log_directory = "log"
    if sorted_papers:
        save_stats_to_csv(
            sorted_papers, 
            get_unique_filepath(log_directory, "top_papers_autosearch.csv"), 
            ["title", "citations", "authors", "url"]
        )
    
    # Save all productive years
    save_counter_to_csv(
        year_counts, 
        log_directory, 
        "productive_years_autosearch.csv", 
        ["Year", "Article_Count"]
    )
    
    # Save all prolific authors
    save_counter_to_csv(
        author_counts, 
        log_directory, 
        "prolific_authors_autosearch.csv", 
        ["Author", "Article_Count"]
    )

    print(f"\n{Fore.WHITE}--- Overall Summary ---")
    print(f"{Fore.GREEN}Total Articles Analyzed: {Style.BRIGHT}{total_articles}")
    print(f"{Fore.GREEN}Unique Articles Found: {Style.BRIGHT}{len(processed_titles)}")

    display_top_items("Most Productive Years", year_counts)
    display_top_items("Most Prolific Authors", author_counts)

    print(f"\n{Fore.YELLOW}--- Top {5} Most Cited Papers ---{Style.RESET_ALL}")
    if not sorted_papers:
        print(f"{Fore.WHITE}No papers with citation data found.")
    else:
        for i, paper in enumerate(sorted_papers[:5]):
            title = paper['title']
            if len(title) > 70:
                title = title[:67] + "..."
            
            print(f"{Fore.WHITE}{i+1}. {Fore.GREEN}{title} \n   {Style.DIM}{Fore.WHITE}└─ Citations: {Style.BRIGHT}{paper['citations']}{Style.RESET_ALL}")

def analyze_llama_csv_results(folder_path):
    """
    Analyzes the CSV file generated by the AI filter.
    This function now takes the folder_path as an argument.
    """
    display_header("AI Filter Results Statistics")

    csv_file = os.path.join(folder_path, "llama_filtered_articles.csv")
    if not os.path.exists(csv_file):
        print(f"{Fore.RED}❌ File '{csv_file}' not found. Please run the AI filter first.")
        return

    year_counts = Counter()
    author_counts = Counter()
    all_papers = []
    processed_titles = set()
    total_articles = 0
    
    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            total_articles += 1
            title = row.get("Title", "").strip()
            if not title or title.upper() == "N/A" or title in processed_titles:
                continue

            processed_titles.add(title)

            if row.get("Year"): year_counts[row["Year"]] += 1
            if row.get("Authors"): author_counts.update([author.strip() for author in row["Authors"].split(';')])
            
            try:
                all_papers.append({
                    "title": title,
                    "citations": int(row.get("Citations", 0)),
                    "authors": row.get("Authors", "N/A"),
                    "url": row.get("URL", "N/A")
                })
            except (ValueError, TypeError):
                continue
    
    sorted_papers = sorted(all_papers, key=lambda p: p['citations'], reverse=True)

    # ==============================================================================
    # MODIFICATION: Save ALL data for years, authors, and articles
    # ==============================================================================
    log_directory = "log"
    if sorted_papers:
        save_stats_to_csv(
            sorted_papers, 
            get_unique_filepath(log_directory, "top_papers_filtered.csv"), 
            ["title", "citations", "authors", "url"]
        )

    # Save all productive years
    save_counter_to_csv(
        year_counts, 
        log_directory, 
        "productive_years_filtered.csv", 
        ["Year", "Article_Count"]
    )
    
    # Save all prolific authors
    save_counter_to_csv(
        author_counts, 
        log_directory, 
        "prolific_authors_filtered.csv", 
        ["Author", "Article_Count"]
    )
    
    print(f"\n{Fore.WHITE}--- Overall Summary ---")
    print(f"{Fore.GREEN}Total Articles Approved by AI: {Style.BRIGHT}{total_articles}")
    print(f"{Fore.GREEN}Unique Articles Found: {Style.BRIGHT}{len(processed_titles)}")

    display_top_items("Most Productive Years", year_counts)
    display_top_items("Most Prolific Authors", author_counts)

    print(f"\n{Fore.YELLOW}--- Top {5} Most Cited Papers ---{Style.RESET_ALL}")
    if not sorted_papers:
        print(f"{Fore.WHITE}No papers with citation data found.")
    else:
        for i, paper in enumerate(sorted_papers[:5]):
            title = paper['title']
            if len(title) > 70:
                title = title[:67] + "..."
            
            print(f"{Fore.WHITE}{i+1}. {Fore.GREEN}{title} \n   {Style.DIM}{Fore.WHITE}└─ Citations: {Style.BRIGHT}{paper['citations']}{Style.RESET_ALL}")

# Example of how you would call the functions (unchanged)
# if __name__ == '__main__':
#     analyze_autosearch('results')
#     analyze_llama_csv_results('llama_filtered')